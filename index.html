<!DOCTYPE html>
<html>

<head>
    <title>DigiRL</title>
    <link rel="icon" href="website/images/icon/icon.png" type="image/icon type">
 
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels@2.0.0"></script>
    <script
        src="https://cdn.jsdelivr.net/npm/chartjs-plugin-annotation@3.0.1/dist/chartjs-plugin-annotation.min.js"></script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="website/css/bulma.min.css">
    <link rel="stylesheet" href="website/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="website/css/bulma-slider.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="./website/javascript/bulma-carousel.min.js"></script>
    <script src="./website/javascript/bulma-slider.min.js"></script>

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p"
        crossorigin="anonymous"></script>

    <link href="https://unpkg.com/tabulator-tables@5.5.2/dist/css/tabulator_bootstrap4.min.css" rel="stylesheet">
    <script type="text/javascript" src="https://unpkg.com/tabulator-tables@5.5.2/dist/js/tabulator.min.js"></script>
    <!-- <script src="website/javascript/peity-vanilla.js"></script> -->

    <script src="website/javascript/benchmark_table.js" type="module"></script>
    <script src="website/javascript/success_rate_vs_k_vis.js" type="module"></script>
    <script src="website/javascript/success_rate_vs_k_vis2.js" type="module"></script>
    <script src="website/javascript/feedback_success_rate_vis.js" type="module"></script>
    <script src="website/javascript/feedback_provider_efficacy.js" type="module"></script>
    <script src="website/javascript/demos.js" type="module"></script>

    <link rel="stylesheet" href="website/css/index.css">

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C7GJ4FYMY9"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-C7GJ4FYMY9');

    </script>

    

    <noscript>
        <p><img alt="Clicky" width="1" height="1" src="//in.getclicky.com/101339888ns.gif" /></p>
    </noscript>
</head>

<body>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title publication-title">
                            <img src="website/images/icon/icon.png" alt="logo" width="60" height="60" />
                            Improving Neuron-level Interpretability with White-box Language Models
                        </h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://www.jackgethome.com/">Hao Bai</a><sup>1,2*</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://aviralkumar2907.github.io/">Yi Ma</a><sup>1,3</sup>
                            </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup> UC Berkeley,</span>
                            <span class="author-block"><sup>2</sup> UIUC,</span>
                            <span class="author-block"><sup>3</sup> HKU</span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><small>*Work done at UC Berkeley</small></span>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                                    <a href="https://arxiv.org/abs/2406.11896" class="btn btn-outline-dark"
                                        role="button">&#128221;
                                        Paper</a> &nbsp;&nbsp;

                                </span>
                                <!-- Code Link. -->
                                <span class="link-block">
                                    <a href="https://github.com/DigiRL-agent/digirl" class="btn btn-outline-dark"
                                        role="button">&#128187;
                                        Code</a> &nbsp;&nbsp;

                                </span>
                                <!-- Dataset Link. -->
                                <span class="link-block">

                                    <a href="https://drive.google.com/drive/folders/14Iu6lAHePQ2qG0ghYkVG1RG6RUu7e2Hz?usp=sharing"
                                        class="btn btn-outline-dark" role="button">&#128194;
                                        Data</a>
                            </div>
                        </div>

                        <!-- <h2 class="subtitle" style="text-align: left;">
                            <b>MINT benchmark</b> measures LLMs' ability to solve tasks with multi-turn interactions
                            by
                            (1) using tools and (2) leveraging natural language feedback.
                        </h2> -->
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section" id="propaganda">
        <div class="container is-max-desktop">
            <div class="is-centered has-text-centered">
                <div class="custom-column-large">
                    <div class="is-centered custom-column-large">
                        <h2 class="title is-3">Demo</h2>
                        <!-- add an image from /Users/mac/Desktop/crate-lm.github.io/website/images/teaser.jpg -->
                    </div>
                </div>
                <img src="website/images/teaser.jpg" alt="teaser" style="max-width: 100%; height: auto;">
                <small>Instances are systematically identified where the interpretability of <strong>CRATE</strong> (ours, <em>row 1</em>) outperforms GPT-2 (<em>row 2</em>). For each neuron (<em>rounded box</em>), we show two top activated text excerpts (<em>excerpt</em> <strong>1</strong> and <strong>2</strong>) and one randomly activated excerpt (<em>excerpt</em> <strong>3</strong>). Results show that <strong>CRATE</strong> consistently activates <em>on and only on</em> semantically relevant text excerpts (first two excerpts), leading to more precise explanations predicted by agents like Mistral.</s>
            </div>
        </div>
    </section>

    <section class="section" id="abstract">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            Neurons in auto-regressive language models like GPT-2 can be interpreted by analyzing their activation patterns. Recent studies have shown that techniques such as dictionary learning, a form of post-hoc sparse coding, enhance this neuron-level interpretability.
                            In our research, we are driven by the goal to fundamentally improve neural network interpretability by embedding sparse coding directly within the model architecture, rather than applying it as an afterthought.
                            In our study, we introduce a white-box transformer-like architecture named Coding RAte TransformEr (CRATE), explicitly engineered to capture sparse, low-dimensional structures within data distributions. 
                            Our comprehensive experiments showcase significant improvements (up to 103% relative improvement) in neuron-level interpretability across a variety of evaluation metrics.
                            Detailed investigations confirm that this enhanced interpretability is steady across different layers irrespective of the model size, underlining CRATE's robust performance in enhancing neural network interpretability.
                            Further analysis shows that CRATE's increased interpretability comes from its enhanced ability to consistently and distinctively activate on relevant tokens. These findings point towards a promising direction for creating white-box foundation models that excel in neuron-level interpretation.
                        </p>
                    </div>
                </div>
            </div>
            <!--/ Abstract. -->
        </div>
    </section>

    <section class="section" id="interaction-framework">
        <div class="container is-max-desktop">

            <div class="columns is-full-width">

                <!-- Visual Effects. -->
                <div class="column">
                    <div class="text-justified">
                        <h2 class="title is-3">CRATE Language Model: Improved Neuron-level Interpretability</h2>
                        <h3>Designing the Architecture</h3>
                        <div class="text-justified" id="tool-augmented">
                            We apply the <a href="https://arxiv.org/abs/2306.01129">original CRATE architecture</a> to the <i>next-token prediction task</i>. This leads to several modifications in the model architecture:
                            <ul>
                              <li><b>(i)</b> We apply a <b>causal mask</b> to ensure the model does not see tokens after the current one.</li>
                              <li><b>(ii)</b> The <b>embedding layer and heads</b> are adjusted to accommodate the language vocabulary.</li>
                            </ul>
                            <p>Additionally, we aim to interpret neurons on the next-token prediction task and compare them directly to GPT-2. Since GPT-2's neuron-level interpretation focuses on the hidden states in the MLP block, we introduce a key adjustment:</p>
                            <ul>
                              <li><b>(iii)</b> We <b>over-parameterize</b> (increase the hidden dimension) of the <tt>ISTA</tt> block to match GPT-2's MLP block.</li>
                            </ul>
                            
                        </div>
                        <img src="website/images/arch.jpg" alt="teaser" style="max-width: 100%; height: auto;">
                        <br><br>
                        <h3> How good is CRATE-LM?</h3>
                        CRATE achieves markedly improved and more steady neuron-level interpretability across layers compared to GPT-2, applicable across a wide range of model sizes.
                        <br>
                        <img src="website/images/interp-scores-overall.jpg" alt="teaser" 
                            style="margin: 0 auto; display: block; max-width: 1000px; width: 100%; height: auto;" />
                            We observe that the interpretability of CRATE comprehensively outperforms GPT-2 on all metrics for L∈{2,3,6,12}. When averaging the mean interpretability across all metrics, CRATE outperforms GPT-2 up to strikingly 103% relative improvement under the OpenAI Random-only metric when L=6. 
                        <br><br>
                        <h3>Layer-wise Interpretability Analysis</h3>
                        <div class="text-justified" id="tool-augmented">
                            Layer-wise interpretability analysis demonstrates that CRATE achieves higher interpretability than GPT-2 on almost all layers.
                        </div>
                        <br>
                        <div style="text-align:center;">
                            <img src="website/images/interp-scores.jpg" alt="teaser" 
                                style="margin: 0 auto; display: block; max-width: 1000px; width: 100%; height: auto;" />
                        </div>
                    </div>

                </div>
                <!--/ Visual Effects. -->

            </div>
    </section>

    <section class="section" id="evaluation">
        <div class="container is-max-desktop">

            <div class="columns is-full-width">

                <!-- Visual Effects. -->
                <div class="column">
                    <div class="content">
                        <h3 class="title is-3">Evaluation Metrics</h3>
                        <p>
                            In order to quantitatively evaluate the interpretability of the neuron activations, we adopt the large language model-based algorithm introduced in <a href="https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html">Bills et al. (OpenAI)</a> and <a href="https://transformer-circuits.pub/2023/monosemantic-features">Bricken et al. (Anthropic)</a>:
                        </p>

                        <div style="text-align:center;">
                            <img src="website/images/interp-eval.jpg" alt="teaser" 
                                style="margin: 0 auto; display: block; max-width: 1000px; width: 100%; height: auto;" />
                        </div>

                        <p>
                            In practice, we adopt three evaluation metrics: two from OpenAI (<i>top-and-random</i> and <i>random-only</i>) and one from Anthropic. We adopt the official implementation from <a href="https://github.com/openai/automated-interpretability">Wu et al.</a>.
                        </p>


                    </div>
                </div>
            </div>
    </section>

    <section class="section" id="evaluation">
        <div class="container is-max-desktop">
            <div class="columns is-full-width">
                <div class="column">
                    <div class="content">
                        <h3>Misc</h3>
                        <h4>Re the Charts &#128200 </h4>
                        Try clicking on the legend of the charts!
                    </div>
                    <div class="content">
                        <h4>Re the Icon <img src="website/images/icon/icon.png" alt="logo" width="30" height="30" /></h4>
                        <p>
                            <b>Infinity:</b> Our environment is open-ended, which can be easily generalized to infinite open-ended tasks sets with our open-ended evaluator.
                            <br>
                            <b>Loop:</b> We use online reinforcement learning, which is closed-loop: the agent interacts with the environment and learns from its own trials and errors.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>@article{bai2024digirl,
title={DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning},
author={Bai, Hao and Zhou, Yifei and Cemri, Mert and Pan, Jiayi and Suhr, Alane and Levine, Sergey and Kumar, Aviral},
journal={arXiv preprint arXiv:2406.11896},
year={2024}
}</code></pre>
        </div>
    </section>

    <footer class="footer">
        <div align="center" class="container">
            <div class="columns is-centered">
                <div class="content is-small">
                    This website templated is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">here</a> and <a href="https://xwang.dev/mint-bench/">here</a>.
                </div>
            </div>
        </div>
    </footer>

</body>


</html>
