<!DOCTYPE html>
<html>

<head>
    <title>CRATE-LM</title>
    <link rel="icon" href="website/images/icon/icon.png" type="image/icon type">
 
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels@2.0.0"></script>
    <script
        src="https://cdn.jsdelivr.net/npm/chartjs-plugin-annotation@3.0.1/dist/chartjs-plugin-annotation.min.js"></script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="website/css/bulma.min.css">
    <link rel="stylesheet" href="website/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="website/css/bulma-slider.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="./website/javascript/bulma-carousel.min.js"></script>
    <script src="./website/javascript/bulma-slider.min.js"></script>

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p"
        crossorigin="anonymous"></script>

    <link href="https://unpkg.com/tabulator-tables@5.5.2/dist/css/tabulator_bootstrap4.min.css" rel="stylesheet">
    <script type="text/javascript" src="https://unpkg.com/tabulator-tables@5.5.2/dist/js/tabulator.min.js"></script>
    <!-- <script src="website/javascript/peity-vanilla.js"></script> -->

    <script src="website/javascript/benchmark_table.js" type="module"></script>
    <script src="website/javascript/success_rate_vs_k_vis.js" type="module"></script>
    <script src="website/javascript/success_rate_vs_k_vis2.js" type="module"></script>
    <script src="website/javascript/feedback_success_rate_vis.js" type="module"></script>
    <script src="website/javascript/feedback_provider_efficacy.js" type="module"></script>
    <script src="website/javascript/demos.js" type="module"></script>

    <link rel="stylesheet" href="website/css/index.css">

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C7GJ4FYMY9"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-C7GJ4FYMY9');

    </script>

    

    <noscript>
        <p><img alt="Clicky" width="1" height="1" src="//in.getclicky.com/101339888ns.gif" /></p>
    </noscript>
</head>

<body>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title publication-title">
                            <img src="website/images/icon/icon.png" alt="logo" width="60" height="60" />
                            Improving Neuron-level Interpretability with White-box Language Models
                        </h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://www.jackgethome.com/">Hao Bai</a><sup>1,2*</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://people.eecs.berkeley.edu/~yima/">Yi Ma</a><sup>1,3</sup>
                            </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup> UC Berkeley,</span>
                            <span class="author-block"><sup>2</sup> UIUC,</span>
                            <span class="author-block"><sup>3</sup> HKU</span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><small>*Work done at UC Berkeley</small></span>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                                    <a href="https://arxiv.org/abs/2406.11896" class="btn btn-outline-dark"
                                        role="button">&#128221;
                                        Paper</a> &nbsp;&nbsp;

                                </span>
                                <!-- Code Link. -->
                                <span class="link-block">
                                    <a href="https://github.com/crate-lm/crate-lm" class="btn btn-outline-dark"
                                        role="button">&#128187;
                                        Code</a> &nbsp;&nbsp;

                                </span>
                            </div>
                        </div>

                        <!-- <h2 class="subtitle" style="text-align: left;">
                            <b>MINT benchmark</b> measures LLMs' ability to solve tasks with multi-turn interactions
                            by
                            (1) using tools and (2) leveraging natural language feedback.
                        </h2> -->
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section" id="propaganda">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <div class="custom-column-large">
                        <div class="is-centered custom-column-large">
                            <h2 class="title is-3">Demo</h2>
                            <!-- add an image from /Users/mac/Desktop/crate-lm.github.io/website/images/teaser.jpg -->
                        </div>
                    </div>
                <img src="website/images/teaser.png" alt="teaser" style="max-width: 100%; height: auto;">
                <a style="color:gray"> Instances are systematically identified where the interpretability of <strong>CRATE</strong> (ours, <em>row 1</em>) outperforms GPT-2 (<em>row 2</em>). For each neuron (<em>rounded box</em>), we show two top activated text excerpts (<em>excerpt</em> <strong>1</strong> and <strong>2</strong>) and one randomly activated excerpt (<em>excerpt</em> <strong>3</strong>). Results show that <strong>CRATE</strong> consistently activates <em>on and only on</em> semantically relevant text excerpts (first two excerpts), leading to more precise explanations predicted by agents like Mistral. </a>
                </div>
            </div>
        </div>
    </section>

    <section class="section" id="abstract">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            Neurons in auto-regressive language models like GPT-2 can be interpreted by analyzing their activation patterns. Recent studies have shown that techniques such as dictionary learning, a form of post-hoc sparse coding, enhance this neuron-level interpretability.
                            In our research, we are driven by the goal to fundamentally improve neural network interpretability by embedding sparse coding directly within the model architecture, rather than applying it as an afterthought.
                            In our study, we introduce a white-box transformer-like architecture named Coding RAte TransformEr (CRATE), explicitly engineered to capture sparse, low-dimensional structures within data distributions. 
                            Our comprehensive experiments showcase significant improvements (up to 103% relative improvement) in neuron-level interpretability across a variety of evaluation metrics.
                            Detailed investigations confirm that this enhanced interpretability is steady across different layers irrespective of the model size, underlining CRATE's robust performance in enhancing neural network interpretability.
                            Further analysis shows that CRATE's increased interpretability comes from its enhanced ability to consistently and distinctively activate on relevant tokens. These findings point towards a promising direction for creating white-box foundation models that excel in neuron-level interpretation.
                        </p>
                    </div>
                </div>
            </div>
            <!--/ Abstract. -->
        </div>
    </section>

    <section class="section" id="interaction-framework">
        <div class="container is-max-desktop">
            <div class="text-justified">
                <h2 class="title is-3">The CRATE Language Model: Improved Neuron-level Interpretability</h2>
                <h3>Architecture</h3>
                <div class="text-justified" id="tool-augmented">
                    We apply the <a href="https://arxiv.org/abs/2306.01129">original CRATE architecture</a> to the <i>next-token prediction task</i> proposed in <a href=https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2</a>. This leads to several modifications in the model architecture:
                    <br> 1. We apply a <b>causal mask</b> to ensure the model does not see tokens after the current one.
                    <br> 2. We <b>over-parameterize</b> (increase the hidden dimension) of the <tt>ISTA</tt> block to match GPT-2's MLP block.
                    <br> These modifications result in the block architecture below:
                </div>
                <br>
                <div class="container is-max-desktop">
                    <div class="columns is-centered has-text-centered">
                        <div class="column is-four-fifths">
                            <img src="website/images/arch.jpg" alt="teaser" 
                                style="margin: 0 auto; display: block; max-width: 1000px; width: 80%; height: auto;" />
                            <p style="color:gray">Block architecture for the CRATE language model.</p>
                        </div>
                    </div>
                </div>
                <br>
                <div class="text-justified" id="tool-augmented"></div>
                    Despite the change of the block architecture, the <b>embedding layer and heads</b> are adjusted to accommodate the language vocabulary. Previously the embedding layer and head were the same as the <a href="https://arxiv.org/abs/2010.11929">Visual Transformer</a>.
                </div>
                <br><br>
                <h3> Learning Process </h3>
                The process starts with random token representations. Through successive layers, the representations are <b>compressed</b> to align with the axis via the MSSA block, forming intermediate representations are semantically more consistent among relevant tokens. This is then refined by <b>sparse coding</b> (the ISTA block) to produce the final representations that align on incoherent axes, leading to semantically more specified token representations. Repeated across layers, this culminates in distinct token representations aligned on unique semantic axes.
                <br><br>
                <div class="container is-max-desktop">
                    <div class="columns is-centered has-text-centered">
                        <div class="column is-four-fifths">
                            <img src="website/images/learning-process.png" alt="teaser" 
                                style="margin: 0 auto; display: block; max-width: 1000px; width: 100%; height: auto;" />
                            <p style="color:gray">CRATE iteratively compresses (MSSA block) and sparsifies (ISTA block) the token representations (colored points) across its layers from 1 to L, transforming them into parsimonious representations aligned on axes (colored lines) with distinct semantic meanings.</p>
                        </div>
                    </div>
                </div>
                <br><br>
                <h3> Performance </h3>
                We pre-train CRATE-Base and GPT2-Base on <a href="https://huggingface.co/datasets/monology/pile-uncopyrighted">the Pile dataset (uncopyrighted)</a>. The training recipe is similar to <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2</a>, please check our paper for details! 
                <br> Key takeaways of performance experiments:
                <br> 1. Both training and validation loss curve of CRATE-Base on the Pile dataset converges well.
                <br> 2. The performance of CRATE is close to GPT-2 across all model sizes.
                <div class="container is-max-desktop">
                    <div class="columns is-centered has-text-centered">
                        <div class="column is-four-fifths">
                            <img src="website/images/loss.png" alt="teaser" 
                                style="margin: 0 auto; display: block; max-width: 1000px; width: 100%; height: auto;" />
                            <p style="color:gray">Left: loss curve when pre-training CRATE-Base and GPT2-Base on the Pile dataset. 
                                <br>Right: Validation loss of CRATE compared to GPT-2 on the Pile dataset, with respect to the model size.</p>
                        </div>
                    </div>
                </div>
                <br> 3. CRATE-LM produces reasonable predictions.
                <div class="container is-max-desktop">
                    <div class="columns is-centered has-text-centered">
                        <div class="column is-four-fifths">
                            <img src="website/images/qual-examples.png" alt="teaser" 
                                style="margin: 0 auto; display: block; max-width: 1000px; width: 100%; height: auto;" />
                            <p style="color:gray">Qualitative examples of predictions made by CRATE and GPT-2. The tokens in blue are considered good. We compare CRATE-Base to GPT2-Base on the next word prediction task.</p>
                        </div>
                    </div>
                </div>
                <br><br>
                <h3> Overall Interpretability </h3>
                CRATE achieves markedly improved and more steady neuron-level interpretability across layers compared to GPT-2, applicable across a wide range of model sizes.
                <br> 1. CRATE comprehensively outperforms GPT-2 on all metrics for L∈{2,3,6,12}. When averaging the mean interpretability across all metrics, CRATE outperforms GPT-2 up to strikingly 103% relative improvement under the OpenAI Random-only metric when L=6.
                <br> 2. The interpretability of CRATE is much more steady than GPT-2 across all model sizes.
                <br>
                <div class="container is-max-desktop">
                    <div class="columns is-centered has-text-centered">
                        <div class="column is-four-fifths">
                            <img src="website/images/interp-scores-overall.jpg" alt="teaser" 
                                style="margin: 0 auto; display: block; max-width: 1000px; width: 100%; height: auto;" />
                            <p style="color:gray">Mean and variance of the average interpretability across layers for different model sizes.</p>
                        </div>
                    </div>
                </div>
                <br><br>
                <h3> Layer-wise Interpretability </h3>
                CRATE achieves higher interpretability than GPT-2 on almost all layers across all model configurations.
                <br>
                <div class="container is-max-desktop">
                    <div class="columns is-centered has-text-centered">
                        <div class="column is-four-fifths">
                            <img src="website/images/interp-scores.jpg" alt="teaser" 
                                style="margin: 0 auto; display: block; max-width: 1000px; width: 100%; height: auto;" />
                            <p style="color:gray">Interpretation scores evaluated using the OpenAI Random-only metric, Top-and-Random metric, and Anthropic metric, respectively. 
                                <br>Top: interpretation scores of CRATE and GPT-2 for L=12. 
                                <br>Middle: interpretation scores of CRATE and GPT-2 for L=6. 
                                <br>Bottom: interpretation scores of CRATE, GPT-2, and GPT-2 with sparse auto-encoder for L∈{1,2,3}. 
                                <br>Variance bars are normalized to 1/10 of its original size.</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section" id="evaluation">
        <div class="container is-max-desktop">

            <div class="columns is-full-width">

                <!-- Visual Effects. -->
                <div class="column">
                    <div class="content">
                        <h3 class="title is-3">Evaluation Metrics</h3>
                        <p>
                            In order to quantitatively evaluate the interpretability of the neuron activations, we adopt the large language model-based algorithm introduced in <a href="https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html">Bills et al. (OpenAI)</a> and <a href="https://transformer-circuits.pub/2023/monosemantic-features">Bricken et al. (Anthropic)</a>:
                        </p>

                        <div style="text-align:center;">
                            <img src="website/images/interp-eval.jpg" alt="teaser" 
                                style="margin: 0 auto; display: block; max-width: 1000px; width: 100%; height: auto;" />
                        </div>

                        <p>
                            In practice, we adopt three evaluation metrics: two from OpenAI (<i>top-and-random</i> and <i>random-only</i>) and one from Anthropic. We adopt the official implementation from <a href="https://github.com/openai/automated-interpretability">Wu et al.</a>.
                        </p>


                    </div>
                </div>
            </div>
    </section>

    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>@article{bai2024improving,
title={Improving Neuron-level Interpretability with White-box Language Models},
author={Bai, Hao and Ma, Yi},
journal={arXiv preprint arXiv:2406.11896},
year={2024}
}</code></pre>
        </div>
    </section>

    <footer class="footer">
        <div align="center" class="container">
            <div class="columns is-centered">
                <div class="content is-small">
                    This website templated is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">here</a> and <a href="https://xwang.dev/mint-bench/">here</a>.
                </div>
            </div>
        </div>
    </footer>

</body>


</html>
